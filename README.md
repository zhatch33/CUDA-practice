# CUDA-practice
Documenting my learning with Numba/CUDA

I've been working with higher level tensor libraries (PyTorch, Keras, TensorFlow) for a while now and have implemented a pretty wide set of different deep learning architectures. I've also been scaling up the size of the input data and models which I am working with significantly, necessitating greater usage of distributed model training as well as distributed/GPU accelerated methods for data maninpulation and processing pipelines, such as the RAPIDS framework (cuDF, dask-CUDA, etc.), for accelerating and parallelizing massive workloads. As the scale of the data and models continues to increase, and the code I am writing becomes more and more performance critical, I believe I will find it helpful to get some hardcore experience "closer to the metal", so-to-speak, learning at a low-level how GPU accelerated libraries manage memory and task scheduling when balancing workloads between the GPU and CPU. 

My ultimate goal for this exercise, in addition to coming away with a better understanding of writing optimized GPU code, is to familiarize myself with the mathematics and implementation of the different methods applied across deep learning models, in order to enrich my theoretical understanding of them to the extent that I can get more creative with my own custom implementations and methods. I would love to be able to implement methods which I read about in papers which have not made it to popular frameworks yet, as well as try experiments which answer questions I have had for myself - for example, has anyone ever tried to treat the sizes of the layers themselves as learnable parameters and set them as optimization targets during training? I could probably look this up right now, but if I had an in-depth understanding of CUDA programming as well as a ready-made custom library of deep learning kernels implemented in Numba-CUDA which I could access, I could throw together an experiment and try for myself, which seems like the more fun option to have.

One thing you may notice of the code in this repo is that it is not actually written in pure CUDA; instead, the relevant parts of the code (GPU kernel implementations, etc.) are written in Numba-CUDA. The justification for this is two-fold:

1. It is my understanding that, in the scope of the CUDA functionality useful for deep learning, Numba-CUDA and CUDA are essentially interchangeable; the very few core functionalities contained in CUDA which are not replicated in Numba (dynamic parallelism, etc) are presently never really used in the implementations of deep learning algorithms. In addition, I have come to understand that the level of abstraction in Numba-CUDA is basically identical to that of CUDA; while its implementation contains slight differences which make it more "pythonic" compared to pure CUDA (IE it allows you to avoid direct use of pointers for the most part), it requires no less involvement from the programmer in terms of thinking through how memory is allocated on the GPU and CPU, the scheduling of parallelism, synchronization of tasks, sharing data between host and device, etc. Thus I do not think that substituting Numba-CUDA for pure CUDA will be any constraint on my actual learning objectives; the effort of learning the principles and implementations of low-level GPU programming will be the same.
2. While I do have experience and a familiarity with the principles of C/C++, I have not worked extensively with either one in some time, and I program in python literally every day. Opting for Numba-CUDA instead of pure CUDA is the best way to drop the work right into my current workflow and tech stack so I can get right to the fun parts without having to deal with the annoying parts of setting up my C environments and getting re-acquinted with the syntax.

In short, I think Numba-CUDA is the perfect choice for what I'm trying to do. If you stumble onto this repo and you're coming from a similar place, it's probably a good fit for you too. Of course, if you think I'm overlooking something important, definitely let me know! Otherwise, I'll stick with this approach until I start diving into computational fluid dynamics or something equally intense. 

I am running / developing this code on a series of different AWS EC2 instances that I'm connecting to, using the most recent Amazon Deep Learning AMI as my development environment; the code should be runnable from any machine with access to an NVIDIA GPU compatible with Numba version 0.61.0. On my end I'll probably be floating in and out of larger and smaller instances in order to play around with memory management and kernel sizes, experimenting with how different kernel parameters and impact runtime, GPU saturation, etc. across different amounts of GPU memory and cluster sizes.

Fair warningâ€”the repo might look messy at first. It'll mostly be snippets and notebook code from my EC2 experimentation sessions, saved here as I figure out basic kernel implementations and play around with deep learning primitives. As I learn more, I'll tidy things up, add better documentation, and modularize kernels for easier reuse in bigger models. 

